{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install yfinance pandas numpy scipy statsmodels tqdm newsapi gnews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LL44uB9yaZH",
        "outputId": "61c771cf-8a6c-45aa-e4d9-d8c1861bf14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: newsapi in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
            "Collecting gnews\n",
            "  Downloading gnews-0.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Collecting feedparser~=6.0.2 (from gnews)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dnspython (from gnews)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.1)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.2->gnews)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Downloading gnews-0.4.2-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=a53efa5a15d4d396def650c79b91bbf6889ab28d458eecb2d637b0cfda1c3580\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, dnspython, gnews\n",
            "Successfully installed dnspython-2.7.0 feedparser-6.0.11 gnews-0.4.2 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------ Fundamentals ------------------------------- #\n",
        "def fetch_fundamentals(ticker: str) -> dict:\n",
        "    \"\"\"Fetch multiple fundamental ratios from Yahoo Finance.\"\"\"\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return {\n",
        "            \"de_ratio\": info.get(\"debtToEquity\"),         # Debt-to-Equity\n",
        "            \"current_ratio\": info.get(\"currentRatio\"),    # Current Ratio\n",
        "            \"quick_ratio\": info.get(\"quickRatio\"),        # Quick Ratio\n",
        "            \"roa\": info.get(\"returnOnAssets\"),            # Return on Assets\n",
        "            \"roe\": info.get(\"returnOnEquity\"),            # Return on Equity\n",
        "            \"profit_margin\": info.get(\"profitMargins\"),   # Profit Margin\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching fundamentals for {ticker}: {e}\")\n",
        "        return {\n",
        "            \"de_ratio\": None,\n",
        "            \"current_ratio\": None,\n",
        "            \"quick_ratio\": None,\n",
        "            \"roa\": None,\n",
        "            \"roe\": None,\n",
        "            \"profit_margin\": None,\n",
        "        }\n",
        "\n",
        "# ---------------------------- Feature Engineering --------------------------- #\n",
        "def compute_simple_features(df: pd.DataFrame, ticker: str, fundamentals: dict) -> pd.DataFrame:\n",
        "    \"\"\"Compute volatility, drawdowns, returns, and attach fundamentals.\"\"\"\n",
        "    df = df.copy().sort_index()\n",
        "    close = df[\"Adj Close\"]\n",
        "\n",
        "    feats = pd.DataFrame(index=df.index)\n",
        "    feats[\"date\"] = df.index\n",
        "    feats[\"ticker\"] = ticker\n",
        "    feats[\"close\"] = close\n",
        "\n",
        "    # Volatility\n",
        "    feats[\"vol_5d\"] = close.pct_change().rolling(5).std()\n",
        "    feats[\"vol_20d\"] = close.pct_change().rolling(20).std()\n",
        "    feats[\"vol_60d\"] = close.pct_change().rolling(60).std()\n",
        "\n",
        "    # Max drawdown (60d)\n",
        "    rolling_max = close.rolling(60, min_periods=1).max()\n",
        "    feats[\"drawdown_60d\"] = close / rolling_max - 1.0\n",
        "\n",
        "    # Add fundamentals (constant across rows for this ticker)\n",
        "    for k, v in fundamentals.items():\n",
        "        feats[k] = v\n",
        "\n",
        "    # Previous returns (trend features)\n",
        "    feats[\"prev_return_5d\"] = close.pct_change(5)\n",
        "    feats[\"prev_return_20d\"] = close.pct_change(20)\n",
        "    feats[\"prev_return_60d\"] = close.pct_change(60)\n",
        "\n",
        "    return feats.reset_index(drop=True)\n",
        "\n",
        "# ------------------------------- Main runner -------------------------------- #\n",
        "def main_multiple_tickers(tickers, period=\"2y\", output_file=\"all_tickers_features.csv\"):\n",
        "    all_features = []\n",
        "\n",
        "    for ticker in tqdm(tickers, desc=\"Processing tickers\"):\n",
        "        print(f\"\\nFetching data for {ticker}â€¦\")\n",
        "        try:\n",
        "            # Historical prices\n",
        "            df = yf.download(ticker, period=period, interval=\"1d\", auto_adjust=False)\n",
        "            if df.empty:\n",
        "                print(f\"Warning: No data for {ticker}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Fundamentals\n",
        "            fundamentals = fetch_fundamentals(ticker)\n",
        "\n",
        "            # Features\n",
        "            features = compute_simple_features(df, ticker, fundamentals)\n",
        "\n",
        "            # Future returns (for supervised labels)\n",
        "            features['future_return_5d'] = features['close'].shift(-5) / features['close'] - 1\n",
        "            features['future_return_20d'] = features['close'].shift(-20) / features['close'] - 1\n",
        "            features['future_return_60d'] = features['close'].shift(-60) / features['close'] - 1\n",
        "\n",
        "            # Labels: 1 if drop > 10%, else 0\n",
        "            drop_threshold = 0.10\n",
        "            features['label_5d'] = (features['future_return_5d'] < -drop_threshold).astype(int)\n",
        "            features['label_20d'] = (features['future_return_20d'] < -drop_threshold).astype(int)\n",
        "            features['label_60d'] = (features['future_return_60d'] < -drop_threshold).astype(int)\n",
        "\n",
        "            all_features.append(features)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {ticker}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if all_features:\n",
        "        final_df = pd.concat(all_features, ignore_index=True)\n",
        "        final_df.to_csv(output_file, index=False)\n",
        "        print(f\"\\nâœ… Saved {final_df.shape[0]} rows to {output_file}\")\n",
        "    else:\n",
        "        print(\"âŒ No data processed.\")\n",
        "\n",
        "# ------------------------------- Run Example -------------------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    tickers_list = [\n",
        "        # Technology\n",
        "        \"AAPL\", \"MSFT\", \"NVDA\", \"GOOG\", \"META\", \"ORCL\", \"IBM\", \"ADBE\", \"INTC\", \"CSCO\",\n",
        "        # Financials\n",
        "        \"JPM\", \"V\", \"BRK-B\", \"GS\", \"BAC\", \"MA\",\n",
        "        # Consumer Staples\n",
        "        \"KO\", \"PEP\", \"PG\", \"WMT\",\n",
        "        # Consumer Discretionary\n",
        "        \"AMZN\", \"DIS\", \"NFLX\", \"TSLA\",\n",
        "        # Industrials\n",
        "        \"BA\", \"CAT\", \"GE\",\n",
        "        # Energy\n",
        "        \"XOM\", \"CVX\", \"COP\",\n",
        "        # Healthcare\n",
        "        \"JNJ\", \"UNH\", \"PFE\", \"MRK\",\n",
        "        # Utilities / REITs\n",
        "        \"NEE\", \"PLD\"\n",
        "    ]\n",
        "\n",
        "    main_multiple_tickers(tickers_list, period=\"2y\", output_file=\"all_tickers_features.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1fJFDkQd4gy",
        "outputId": "da1dbe15-3b05-4382-bdaa-15ccd46dd0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing tickers:   0%|          | 0/36 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for AAPLâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:   3%|â–Ž         | 1/36 [00:00<00:31,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for MSFTâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for NVDAâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing tickers:   8%|â–Š         | 3/36 [00:02<00:23,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for GOOGâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  11%|â–ˆ         | 4/36 [00:02<00:23,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for METAâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  14%|â–ˆâ–        | 5/36 [00:03<00:23,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for ORCLâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  17%|â–ˆâ–‹        | 6/36 [00:04<00:23,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for IBMâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  19%|â–ˆâ–‰        | 7/36 [00:05<00:24,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for ADBEâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  22%|â–ˆâ–ˆâ–       | 8/36 [00:06<00:23,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for INTCâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  25%|â–ˆâ–ˆâ–Œ       | 9/36 [00:07<00:25,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for CSCOâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  28%|â–ˆâ–ˆâ–Š       | 10/36 [00:08<00:24,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for JPMâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  31%|â–ˆâ–ˆâ–ˆ       | 11/36 [00:09<00:23,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for Vâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12/36 [00:10<00:21,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for BRK-Bâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/36 [00:11<00:19,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for GSâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 14/36 [00:11<00:17,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for BACâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/36 [00:12<00:16,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for MAâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/36 [00:13<00:15,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for KOâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17/36 [00:14<00:15,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for PEPâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 18/36 [00:14<00:14,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for PGâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 19/36 [00:15<00:14,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for WMTâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 20/36 [00:16<00:14,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for AMZNâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 21/36 [00:17<00:12,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for DISâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 22/36 [00:18<00:12,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for NFLXâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/36 [00:19<00:11,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for TSLAâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 24/36 [00:20<00:09,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for BAâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 25/36 [00:21<00:10,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for CATâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/36 [00:22<00:09,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for GEâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 27/36 [00:23<00:08,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for XOMâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 28/36 [00:23<00:07,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for CVXâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 29/36 [00:24<00:06,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for COPâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 30/36 [00:25<00:05,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for JNJâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 31/36 [00:26<00:04,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for UNHâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 32/36 [00:27<00:03,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for PFEâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/36 [00:28<00:02,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for MRKâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/36 [00:29<00:01,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for NEEâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 35/36 [00:30<00:00,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for PLDâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "Processing tickers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:30<00:00,  1.17it/s]\n",
            "/tmp/ipython-input-3976616489.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  final_df = pd.concat(all_features, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Saved 18108 rows to all_tickers_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUNwOkw7gFOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Input / Output paths\n",
        "INPUT_CSV = \"features_with_ratios.csv\"\n",
        "OUTPUT_CSV = \"all_tickers_features_clean.csv\"\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "\n",
        "# Drop rows with any NA/empty values\n",
        "df_clean = df.dropna()\n",
        "df = df.drop(columns=[\"close\"])\n",
        "# Save cleaned file\n",
        "df_clean.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"âœ… Cleaned data saved to {OUTPUT_CSV}\")\n",
        "print(f\"Original rows: {len(df)}, Cleaned rows: {len(df_clean)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88rkVN0zx5RT",
        "outputId": "a2a810e1-20cf-4522-f6b6-038295f01f4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cleaned data saved to all_tickers_features_clean.csv\n",
            "Original rows: 13788, Cleaned rows: 13788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def add_decayed_sentiment_per_ticker(\n",
        "    df: pd.DataFrame,\n",
        "    ticker_col: str = \"ticker\",\n",
        "    date_col: str = \"date\",\n",
        "    sent_col: str = \"sentiment_score\",\n",
        "    decay: float = 0.8,\n",
        "    scale: float = 20.0,\n",
        "    gap_aware: bool = False  # if True, decay^Î”days across date gaps\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a 'decayed_sentiment' column computed as:\n",
        "        score_t = (score_{t-1} * decay_eff) + sentiment_t * scale\n",
        "    computed independently within each ticker, in date order.\n",
        "    First row per ticker uses score = sentiment * scale.\n",
        "\n",
        "    gap_aware=True -> decay_eff = decay ** Î”days (Î”days >= 1)\n",
        "    gap_aware=False -> decay_eff = decay for every step.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    out[date_col] = pd.to_datetime(out[date_col])\n",
        "\n",
        "    def _per_ticker(g: pd.DataFrame) -> pd.DataFrame:\n",
        "        g = g.sort_values(date_col).copy()\n",
        "        prev_score = 0.0\n",
        "        prev_date = None\n",
        "        scores = []\n",
        "\n",
        "        for _, row in g.iterrows():\n",
        "            s = float(row[sent_col])\n",
        "            if gap_aware and prev_date is not None:\n",
        "                delta_days = max((row[date_col] - prev_date).days, 1)\n",
        "                decay_eff = decay ** delta_days\n",
        "            else:\n",
        "                decay_eff = decay\n",
        "\n",
        "            current = prev_score * decay_eff + s * scale\n",
        "            scores.append(current)\n",
        "            prev_score = current\n",
        "            prev_date = row[date_col]\n",
        "\n",
        "        g[\"decayed_sentiment\"] = scores\n",
        "        return g\n",
        "\n",
        "    out = (out\n",
        "           .sort_values([ticker_col, date_col])\n",
        "           .groupby(ticker_col, group_keys=False)\n",
        "           .apply(_per_ticker))\n",
        "    return out\n",
        "\n",
        "# ---------- Usage on your CSV ----------\n",
        "# Replace with your actual file name\n",
        "input_csv = \"all_tickers_features_clean.csv\"\n",
        "output_csv = \"final training.csv\"\n",
        "\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Compute decayed sentiment per ticker\n",
        "# tweak decay/scale as you like; set gap_aware=True if you want decay across calendar gaps\n",
        "df_out = add_decayed_sentiment_per_ticker(\n",
        "    df,\n",
        "    ticker_col=\"ticker\",\n",
        "    date_col=\"date\",\n",
        "    sent_col=\"sentiment_score\",\n",
        "    decay=0.8,\n",
        "    scale=1,\n",
        "    gap_aware=False\n",
        ")\n",
        "\n",
        "df_out.to_csv(output_csv, index=False)\n",
        "print(f\"âœ… Saved with decayed column -> {output_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv_EGZfUKWeD",
        "outputId": "e2b87964-f79f-42de-aeb4-cd8743589997"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4099677884.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(_per_ticker))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved with decayed column -> final training.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMT85TWlyM3k",
        "outputId": "ff4dfcb8-e1ed-4c1b-9cc9-a45dae45b69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- TRAINING SCRIPT ---------------- #\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- CONFIG ---------------- #\n",
        "CUSTOM_THRESHOLD = 0.2   # lower cutoff â†’ more recall for class 1\n",
        "\n",
        "# ---------------- Load Data ---------------- #\n",
        "df = pd.read_csv(\"final training.csv\", parse_dates=[\"date\"])\n",
        "\n",
        "# âœ… Feature columns (technical + fundamentals + sentiment)\n",
        "feature_cols = [\n",
        "    \"vol_5d\", \"vol_20d\", \"vol_60d\",\n",
        "    \"drawdown_60d\", \"de_ratio\",\n",
        "    \"prev_return_5d\", \"prev_return_20d\", \"prev_return_60d\",\n",
        "    \"decayed_sentiment\"\n",
        "]\n",
        "\n",
        "# âœ… Label columns\n",
        "label_cols = [\"label_5d\", \"label_20d\", \"label_60d\"]\n",
        "\n",
        "# ---------------- Data Cleaning ---------------- #\n",
        "# Only keep columns that actually exist in the CSV\n",
        "missing = [c for c in feature_cols + label_cols if c not in df.columns]\n",
        "if missing:\n",
        "    print(f\"âš ï¸ Warning: Missing columns in dataset: {missing}\")\n",
        "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
        "    label_cols = [c for c in label_cols if c in df.columns]\n",
        "\n",
        "# Drop rows with NaN in required columns\n",
        "df = df.dropna(subset=feature_cols + label_cols).reset_index(drop=True)\n",
        "\n",
        "# ---------------- Features / Labels ---------------- #\n",
        "X = df[feature_cols]\n",
        "y_dict = {label: df[label] for label in label_cols}\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# To store trained models & predicted probabilities\n",
        "models = {}\n",
        "probabilities = pd.DataFrame({\"date\": df[\"date\"], \"ticker\": df[\"ticker\"]})\n",
        "\n",
        "# ---------------- Train per label ---------------- #\n",
        "for label_name in tqdm(label_cols, desc=\"Training labels\"):\n",
        "    y = y_dict[label_name]\n",
        "\n",
        "    # Stratified split to preserve class balance\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Handle class imbalance â†’ weight minority class (1 = risky)\n",
        "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Predict probabilities\n",
        "    y_pred_prob_test = model.predict_proba(X_test)[:, 1]\n",
        "    probabilities[f\"prob_{label_name}\"] = model.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "    # Evaluate with default 0.5 threshold\n",
        "    auc = roc_auc_score(y_test, y_pred_prob_test)\n",
        "    print(f\"\\nðŸ“Š ROC-AUC for {label_name}: {auc:.4f}\")\n",
        "    print(\"Default threshold (0.5):\")\n",
        "    print(classification_report(y_test, (y_pred_prob_test >= 0.5).astype(int)))\n",
        "\n",
        "    # Evaluate with custom threshold\n",
        "    print(f\"\\nCustom threshold ({CUSTOM_THRESHOLD}):\")\n",
        "    print(classification_report(y_test, (y_pred_prob_test >= CUSTOM_THRESHOLD).astype(int)))\n",
        "\n",
        "    # Save model\n",
        "    models[label_name] = model\n",
        "    joblib.dump(model, f\"xgb_model_{label_name}.pkl\")\n",
        "\n",
        "# ---------------- Save Outputs ---------------- #\n",
        "probabilities.to_csv(\"predicted_probabilities.csv\", index=False)\n",
        "print(\"\\nâœ… All models trained and probabilities saved!\")\n",
        "\n",
        "# Save the fitted scaler for inference\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "print(\"âœ… Scaler saved for inference!\")\n"
      ],
      "metadata": {
        "id": "zHhxv9N4q9ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dcccd50-4aab-4dbe-9fc7-045c4d633125"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining labels:   0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:07:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "Training labels:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š ROC-AUC for label_5d: 0.8300\n",
            "Default threshold (0.5):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98      2709\n",
            "           1       0.27      0.39      0.32        49\n",
            "\n",
            "    accuracy                           0.97      2758\n",
            "   macro avg       0.63      0.68      0.65      2758\n",
            "weighted avg       0.98      0.97      0.97      2758\n",
            "\n",
            "\n",
            "Custom threshold (0.2):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95      2709\n",
            "           1       0.11      0.57      0.19        49\n",
            "\n",
            "    accuracy                           0.91      2758\n",
            "   macro avg       0.55      0.75      0.57      2758\n",
            "weighted avg       0.98      0.91      0.94      2758\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:07:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "Training labels:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š ROC-AUC for label_20d: 0.9156\n",
            "Default threshold (0.5):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96      2571\n",
            "           1       0.49      0.69      0.57       187\n",
            "\n",
            "    accuracy                           0.93      2758\n",
            "   macro avg       0.73      0.82      0.77      2758\n",
            "weighted avg       0.94      0.93      0.94      2758\n",
            "\n",
            "\n",
            "Custom threshold (0.2):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.73      0.84      2571\n",
            "           1       0.20      0.91      0.32       187\n",
            "\n",
            "    accuracy                           0.74      2758\n",
            "   macro avg       0.59      0.82      0.58      2758\n",
            "weighted avg       0.94      0.74      0.81      2758\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:07:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "Training labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š ROC-AUC for label_60d: 0.9535\n",
            "Default threshold (0.5):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94      2369\n",
            "           1       0.61      0.85      0.71       389\n",
            "\n",
            "    accuracy                           0.90      2758\n",
            "   macro avg       0.79      0.88      0.82      2758\n",
            "weighted avg       0.92      0.90      0.91      2758\n",
            "\n",
            "\n",
            "Custom threshold (0.2):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.73      0.84      2369\n",
            "           1       0.37      0.96      0.54       389\n",
            "\n",
            "    accuracy                           0.77      2758\n",
            "   macro avg       0.68      0.85      0.69      2758\n",
            "weighted avg       0.90      0.77      0.80      2758\n",
            "\n",
            "\n",
            "âœ… All models trained and probabilities saved!\n",
            "âœ… Scaler saved for inference!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# better explainabilty\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import joblib\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# -------------------- Nearest Trading Day -------------------- #\n",
        "def get_nearest_trading_day(df, target_date):\n",
        "    target_date = pd.to_datetime(target_date)\n",
        "    available_dates = df.index\n",
        "    nearest_date = available_dates[available_dates <= target_date].max()\n",
        "    return nearest_date\n",
        "\n",
        "# -------------------- Fundamentals -------------------- #\n",
        "def fetch_fundamentals(ticker: str) -> dict:\n",
        "    \"\"\"Fetch multiple fundamental ratios from Yahoo Finance.\"\"\"\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return {\n",
        "            \"de_ratio\": info.get(\"debtToEquity\"),\n",
        "            \"current_ratio\": info.get(\"currentRatio\"),\n",
        "            \"quick_ratio\": info.get(\"quickRatio\"),\n",
        "            \"roa\": info.get(\"returnOnAssets\"),\n",
        "            \"roe\": info.get(\"returnOnEquity\"),\n",
        "            \"profit_margin\": info.get(\"profitMargins\"),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching fundamentals for {ticker}: {e}\")\n",
        "        return {k: None for k in [\n",
        "            \"de_ratio\", \"current_ratio\", \"quick_ratio\",\n",
        "            \"roa\", \"roe\", \"profit_margin\"\n",
        "        ]}\n",
        "\n",
        "# -------------------- Feature Calculation -------------------- #\n",
        "def compute_features_for_inference(df: pd.DataFrame, ticker: str, fundamentals: dict, target_date: str) -> dict:\n",
        "    df = df.sort_index()\n",
        "    close = df[\"Adj Close\"]\n",
        "\n",
        "    # align to nearest trading day\n",
        "    if target_date not in close.index.strftime('%Y-%m-%d'):\n",
        "        nearest_date = get_nearest_trading_day(close, target_date)\n",
        "        if pd.isna(nearest_date):\n",
        "            raise ValueError(f\"No trading data available near {target_date} for {ticker}\")\n",
        "        print(f\"Using nearest trading day: {nearest_date.strftime('%Y-%m-%d')} instead of {target_date}\")\n",
        "        target_date = nearest_date.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Volatility\n",
        "    vol_5d = close.pct_change().rolling(5).std()\n",
        "    vol_20d = close.pct_change().rolling(20).std()\n",
        "    vol_60d = close.pct_change().rolling(60).std()\n",
        "\n",
        "    # Max drawdown\n",
        "    rolling_max = close.rolling(60, min_periods=1).max()\n",
        "    drawdown_60d = close / rolling_max - 1.0\n",
        "\n",
        "    # Previous returns\n",
        "    prev_return_5d = close.pct_change(5)\n",
        "    prev_return_20d = close.pct_change(20)\n",
        "    prev_return_60d = close.pct_change(60)\n",
        "\n",
        "    # Extract features for target date\n",
        "    date_idx = close.index.get_loc(pd.to_datetime(target_date))\n",
        "    features = {\n",
        "        \"date\": target_date,\n",
        "        \"ticker\": ticker,\n",
        "        \"vol_5d\": vol_5d.iloc[date_idx],\n",
        "        \"vol_20d\": vol_20d.iloc[date_idx],\n",
        "        \"vol_60d\": vol_60d.iloc[date_idx],\n",
        "        \"drawdown_60d\": drawdown_60d.iloc[date_idx],\n",
        "        \"prev_return_5d\": prev_return_5d.iloc[date_idx],\n",
        "        \"prev_return_20d\": prev_return_20d.iloc[date_idx],\n",
        "        \"prev_return_60d\": prev_return_60d.iloc[date_idx],\n",
        "    }\n",
        "    features.update(fundamentals)\n",
        "    return features\n",
        "\n",
        "def get_ticker_features(ticker: str, target_date: str, lookback_years: int = 2) -> dict:\n",
        "    df = yf.download(ticker, period=f\"{lookback_years}y\", interval=\"1d\", auto_adjust=False)\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"No data fetched for {ticker}.\")\n",
        "    fundamentals = fetch_fundamentals(ticker)\n",
        "    features = compute_features_for_inference(df, ticker, fundamentals, target_date)\n",
        "    return features\n",
        "\n",
        "# -------------------- Load Models -------------------- #\n",
        "models = {\n",
        "    \"label_5d\": joblib.load(\"xgb_model_label_5d.pkl\"),\n",
        "    \"label_20d\": joblib.load(\"xgb_model_label_20d.pkl\"),\n",
        "    \"label_60d\": joblib.load(\"xgb_model_label_60d.pkl\"),\n",
        "}\n",
        "feature_cols = [\n",
        "    \"vol_5d\", \"vol_20d\", \"vol_60d\",\n",
        "    \"drawdown_60d\", \"de_ratio\",\n",
        "    \"current_ratio\", \"quick_ratio\",\n",
        "    \"roa\", \"roe\", \"profit_margin\",\n",
        "    \"prev_return_5d\", \"prev_return_20d\", \"prev_return_60d\"\n",
        "]\n",
        "\n",
        "# -------------------- Creditworthiness Formula -------------------- #\n",
        "def creditworthiness_from_prob(prob: float) -> float:\n",
        "    \"\"\"\n",
        "    Transform risk probability into a creditworthiness score (300â€“850 scale).\n",
        "    \"\"\"\n",
        "    prob = np.clip(prob, 1e-6, 1 - 1e-6)\n",
        "    score = 800 / (1 + np.exp(5 * (prob - 0.5)))  # logistic curve\n",
        "    score = 300 + (score / 800) * 550\n",
        "    return round(score, 2)\n",
        "\n",
        "# -------------------- Main Calculation with Explainability -------------------- #\n",
        "def calculate_creditworthiness_with_explain(features: dict, method: str = \"weighted\"):\n",
        "    df = pd.DataFrame([features])\n",
        "\n",
        "    # Scale\n",
        "    scaler = joblib.load(\"scaler.pkl\")\n",
        "    X_scaled = scaler.transform(df[feature_cols])\n",
        "\n",
        "    probs = {}\n",
        "    shap_metadata = {}\n",
        "\n",
        "    for label, model in models.items():\n",
        "        prob = model.predict_proba(X_scaled)[:, 1][0]\n",
        "        probs[label] = float(prob)  # ensure JSON serializable\n",
        "\n",
        "        # ---- SHAP local explanation ----\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X_scaled)\n",
        "\n",
        "        # Collect metadata for JSON\n",
        "        shap_metadata[label] = {\n",
        "            \"base_value\": float(explainer.expected_value),\n",
        "            \"feature_values\": {\n",
        "                f: float(v) if not np.isnan(v) else None\n",
        "                for f, v in zip(feature_cols, X_scaled[0])\n",
        "            },\n",
        "            \"shap_values\": {\n",
        "                f: float(v) if not np.isnan(v) else None\n",
        "                for f, v in zip(feature_cols, shap_values[0])\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # ---- Aggregate probability ----\n",
        "    if method == \"weighted\":\n",
        "        weights = {\"label_5d\": 0.3, \"label_20d\": 0.4, \"label_60d\": 0.3}\n",
        "        avg_prob = sum(probs[label] * weights[label] for label in probs)\n",
        "    elif method == \"geometric\":\n",
        "        avg_prob = np.prod(list(probs.values())) ** (1 / len(probs))\n",
        "    elif method == \"exponential\":\n",
        "        avg_prob = np.mean(list(probs.values())) ** 1.5\n",
        "    else:\n",
        "        avg_prob = np.mean(list(probs.values()))\n",
        "\n",
        "    creditworthiness = creditworthiness_from_prob(avg_prob)\n",
        "\n",
        "    return creditworthiness, probs, shap_metadata\n",
        "\n",
        "\n",
        "\n",
        "# -------------------- JSON Output -------------------- #\n",
        "def calculate_creditworthiness_with_explain_json(features: dict, method: str = \"weighted\"):\n",
        "    creditworthiness, probs = calculate_creditworthiness_with_explain(features, method)\n",
        "    # ---- Build JSON result ----\n",
        "    result = {\n",
        "        \"ticker\": features[\"ticker\"],\n",
        "        \"date\": features[\"date\"],\n",
        "        \"creditworthiness\": creditworthiness,\n",
        "        \"risk_probs\": probs,\n",
        "        \"shap_explanations\": shap_metadata\n",
        "    }\n",
        "    return json.dumps(result, indent=2)\n",
        "\n",
        "# -------------------- Example Usage -------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    ticker = \"AMZN\"\n",
        "    target_date = \"2025-03-20\"\n",
        "\n",
        "    features = get_ticker_features(ticker, target_date)\n",
        "    result_json = calculate_creditworthiness_with_explain(features, method=\"weighted\")\n",
        "    print(result_json)\n"
      ],
      "metadata": {
        "id": "k1st9DFxjiKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf82d02-00fe-4ae9-d8a2-3a7a0ceef131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/_array_api.py:839: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  array = numpy.asarray(array, order=order, dtype=dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(np.float64(623.16), {'label_5d': 0.2629871070384979, 'label_20d': 0.8717597126960754, 'label_60d': 0.005421677604317665}, {'label_5d': {'base_value': -0.02495281957089901, 'feature_values': {'vol_5d': 0.008297601571907131, 'vol_20d': 0.1792384566677455, 'vol_60d': -0.09668200482468826, 'drawdown_60d': -1.6894718901129662, 'de_ratio': -0.48729005325941716, 'current_ratio': -0.3746794461940098, 'quick_ratio': -0.2998552337251468, 'roa': -0.2773024710618573, 'roe': -0.33141132232538495, 'profit_margin': -0.6032362440684125, 'prev_return_5d': 0.03251073719905767, 'prev_return_20d': -1.616895591929384, 'prev_return_60d': -1.1530990227904674}, 'shap_values': {'vol_5d': -0.03189438208937645, 'vol_20d': 0.5902968645095825, 'vol_60d': -0.5501396059989929, 'drawdown_60d': 0.34279865026474, 'de_ratio': -0.05299476534128189, 'current_ratio': -0.3305078446865082, 'quick_ratio': 0.002138230949640274, 'roa': -0.2952125370502472, 'roe': 0.03989440202713013, 'profit_margin': -0.13787221908569336, 'prev_return_5d': -0.5448597073554993, 'prev_return_20d': 0.04355688393115997, 'prev_return_60d': -0.08075105398893356}}, 'label_20d': {'base_value': -0.011793878860771656, 'feature_values': {'vol_5d': 0.008297601571907131, 'vol_20d': 0.1792384566677455, 'vol_60d': -0.09668200482468826, 'drawdown_60d': -1.6894718901129662, 'de_ratio': -0.48729005325941716, 'current_ratio': -0.3746794461940098, 'quick_ratio': -0.2998552337251468, 'roa': -0.2773024710618573, 'roe': -0.33141132232538495, 'profit_margin': -0.6032362440684125, 'prev_return_5d': 0.03251073719905767, 'prev_return_20d': -1.616895591929384, 'prev_return_60d': -1.1530990227904674}, 'shap_values': {'vol_5d': -0.1110275387763977, 'vol_20d': 0.378787636756897, 'vol_60d': 0.9183067679405212, 'drawdown_60d': 0.7712901830673218, 'de_ratio': 0.48303040862083435, 'current_ratio': 0.009727456606924534, 'quick_ratio': -0.02598842792212963, 'roa': -0.11397994309663773, 'roe': -0.23947106301784515, 'profit_margin': -0.013219411484897137, 'prev_return_5d': 0.10776108503341675, 'prev_return_20d': -0.1564609855413437, 'prev_return_60d': -0.08035481721162796}}, 'label_60d': {'base_value': -0.009642099030315876, 'feature_values': {'vol_5d': 0.008297601571907131, 'vol_20d': 0.1792384566677455, 'vol_60d': -0.09668200482468826, 'drawdown_60d': -1.6894718901129662, 'de_ratio': -0.48729005325941716, 'current_ratio': -0.3746794461940098, 'quick_ratio': -0.2998552337251468, 'roa': -0.2773024710618573, 'roe': -0.33141132232538495, 'profit_margin': -0.6032362440684125, 'prev_return_5d': 0.03251073719905767, 'prev_return_20d': -1.616895591929384, 'prev_return_60d': -1.1530990227904674}, 'shap_values': {'vol_5d': -0.051596466451883316, 'vol_20d': 0.18081438541412354, 'vol_60d': -1.1318169832229614, 'drawdown_60d': -0.98508620262146, 'de_ratio': -0.13947759568691254, 'current_ratio': -0.5839126110076904, 'quick_ratio': 0.05072100833058357, 'roa': -0.326808363199234, 'roe': -0.3901287615299225, 'profit_margin': -0.3140387237071991, 'prev_return_5d': -0.1009807139635086, 'prev_return_20d': 0.021472670137882233, 'prev_return_60d': -1.4314310550689697}}})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gnews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2PdKIRmXPR8",
        "outputId": "0a53a54a-4de2-403a-d8dc-9317d320897d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gnews\n",
            "  Downloading gnews-0.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting feedparser~=6.0.2 (from gnews)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9.3 in /usr/local/lib/python3.12/dist-packages (from gnews) (4.13.4)\n",
            "Collecting dnspython (from gnews)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from gnews) (2.32.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.9.3->gnews) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.9.3->gnews) (4.14.1)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.2->gnews)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (2025.8.3)\n",
            "Downloading gnews-0.4.2-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=f1a6567469d6676ad662475da90ac23df95614c03871e9e0c2327fcbfc49a936\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, dnspython, gnews\n",
            "Successfully installed dnspython-2.7.0 feedparser-6.0.11 gnews-0.4.2 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final_inference.py\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import joblib\n",
        "import shap\n",
        "from gnews import GNews\n",
        "from transformers import pipeline\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# -------------------- Fundamentals -------------------- #\n",
        "def fetch_fundamentals(ticker: str) -> dict:\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return {\n",
        "            \"de_ratio\": info.get(\"debtToEquity\", 0.0),\n",
        "            \"current_ratio\": info.get(\"currentRatio\", 0.0),\n",
        "            \"quick_ratio\": info.get(\"quickRatio\", 0.0),\n",
        "            \"roa\": info.get(\"returnOnAssets\", 0.0),\n",
        "            \"roe\": info.get(\"returnOnEquity\", 0.0),\n",
        "            \"profit_margin\": info.get(\"profitMargins\", 0.0),\n",
        "        }\n",
        "    except Exception:\n",
        "        return {\n",
        "            \"de_ratio\": 0.0, \"current_ratio\": 0.0, \"quick_ratio\": 0.0,\n",
        "            \"roa\": 0.0, \"roe\": 0.0, \"profit_margin\": 0.0\n",
        "        }\n",
        "\n",
        "# -------------------- Sentiment & News -------------------- #\n",
        "def get_company_name(stock_ticker: str) -> str:\n",
        "    try:\n",
        "        company = yf.Ticker(stock_ticker)\n",
        "        return company.info.get(\"longName\", \"\")\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def fetch_company_news(company_name: str, date: str, window: int = 3, max_articles: int = 5) -> list:\n",
        "    try:\n",
        "        google_news = GNews(language=\"en\", country=\"US\", max_results=max_articles)\n",
        "        target_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "        start_date = target_date - timedelta(days=window)\n",
        "        end_date = target_date + timedelta(days=window)\n",
        "\n",
        "        if start_date.date() == end_date.date():\n",
        "            end_date += timedelta(days=1)\n",
        "\n",
        "        google_news.start_date = (start_date.year, start_date.month, start_date.day)\n",
        "        google_news.end_date = (end_date.year, end_date.month, end_date.day)\n",
        "\n",
        "        news_results = google_news.get_news(company_name)\n",
        "        return news_results[:max_articles] if news_results else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def analyze_sentiment_with_hf(news_data: list) -> float:\n",
        "    if not news_data:\n",
        "        return 0.0\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
        "    )\n",
        "    total_score, count = 0, 0\n",
        "    for article in news_data:\n",
        "        try:\n",
        "            text_to_analyze = f\"{article.get('title', '')}. {article.get('description', '')}\"\n",
        "            sentiment = sentiment_pipeline(text_to_analyze)[0]\n",
        "            label, score = sentiment[\"label\"].upper(), sentiment[\"score\"]\n",
        "            mapped = score if label == \"POSITIVE\" else -score if label == \"NEGATIVE\" else 0.0\n",
        "            total_score += mapped\n",
        "            count += 1\n",
        "        except Exception:\n",
        "            continue\n",
        "    return round(total_score / count, 4) if count > 0 else 0.0\n",
        "\n",
        "# -------------------- Decayed Sentiment -------------------- #\n",
        "def sentiment_decay(dates, sentiments, decay=0.8, scale=1.0):\n",
        "    df = pd.DataFrame({\"date\": pd.to_datetime(dates), \"sentiment\": sentiments})\n",
        "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "    decayed_scores, prev_score = [], 0\n",
        "    for _, row in df.iterrows():\n",
        "        s = row[\"sentiment\"]\n",
        "        new_impact = s * scale\n",
        "        current_score = prev_score * decay + new_impact\n",
        "        decayed_scores.append(current_score)\n",
        "        prev_score = current_score\n",
        "    df[\"decayed_score\"] = decayed_scores\n",
        "    return df\n",
        "\n",
        "def compute_sentiment_features(ticker: str, target_date: str, lookback_days: int = 5) -> tuple[float, float]:\n",
        "    company_name = get_company_name(ticker)\n",
        "    if not company_name:\n",
        "        return 0.0, 0.0\n",
        "    dates, sentiments = [], []\n",
        "    for i in range(lookback_days, -1, -1):\n",
        "        day = (pd.to_datetime(target_date) - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
        "        news = fetch_company_news(company_name, day, window=0, max_articles=5)\n",
        "        score = analyze_sentiment_with_hf(news)\n",
        "        dates.append(day)\n",
        "        sentiments.append(score)\n",
        "\n",
        "    df = sentiment_decay(dates, sentiments, decay=0.8, scale=1.0)\n",
        "    decayed_score = float(df[\"decayed_score\"].iloc[-1])\n",
        "    latest_raw_score = float(df[\"sentiment\"].iloc[-1])\n",
        "    return decayed_score, latest_raw_score\n",
        "\n",
        "# -------------------- Technicals -------------------- #\n",
        "def safe_get(series, date):\n",
        "    val = series.loc[date]\n",
        "    if isinstance(val, pd.Series):\n",
        "        return float(val.iloc[-1] if not val.empty else 0.0)\n",
        "    return float(val or 0.0)\n",
        "\n",
        "def compute_features_for_inference(df: pd.DataFrame, fundamentals: dict, target_date: str) -> dict:\n",
        "    df = df.sort_index()\n",
        "    df = df[~df.index.duplicated(keep='last')]\n",
        "    close = df[\"Adj Close\"]\n",
        "    dt_target = pd.to_datetime(target_date)\n",
        "\n",
        "    df_filtered = df[df.index <= dt_target]\n",
        "    if df_filtered.empty:\n",
        "        raise ValueError(f\"No trading data available on or before {target_date}\")\n",
        "    actual_date = df_filtered.index.max()\n",
        "\n",
        "    vol_5d = close.pct_change().rolling(5).std()\n",
        "    vol_20d = close.pct_change().rolling(20).std()\n",
        "    vol_60d = close.pct_change().rolling(60).std()\n",
        "    rolling_max = close.rolling(60, min_periods=1).max()\n",
        "    drawdown_60d = close / rolling_max - 1.0\n",
        "    prev_return_5d = close.pct_change(5)\n",
        "    prev_return_20d = close.pct_change(20)\n",
        "    prev_return_60d = close.pct_change(60)\n",
        "\n",
        "    features = {\n",
        "        \"vol_5d\": safe_get(vol_5d, actual_date),\n",
        "        \"vol_20d\": safe_get(vol_20d, actual_date),\n",
        "        \"vol_60d\": safe_get(vol_60d, actual_date),\n",
        "        \"drawdown_60d\": safe_get(drawdown_60d, actual_date),\n",
        "        \"prev_return_5d\": safe_get(prev_return_5d, actual_date),\n",
        "        \"prev_return_20d\": safe_get(prev_return_20d, actual_date),\n",
        "        \"prev_return_60d\": safe_get(prev_return_60d, actual_date),\n",
        "    }\n",
        "\n",
        "    # Include fundamentals\n",
        "    features.update({k: float(v or 0.0) for k, v in fundamentals.items()})\n",
        "\n",
        "    features[\"date\"] = actual_date.strftime('%Y-%m-%d')\n",
        "    return features\n",
        "\n",
        "def get_ticker_features(ticker: str, target_date: str, lookback_years: int = 2) -> dict:\n",
        "    df = yf.download(ticker, period=f\"{lookback_years}y\", interval=\"1d\", auto_adjust=False, progress=False)\n",
        "    fundamentals = fetch_fundamentals(ticker)\n",
        "    features = compute_features_for_inference(df, fundamentals, target_date)\n",
        "\n",
        "    # Add sentiment\n",
        "    decayed_sentiment, _ = compute_sentiment_features(ticker, features[\"date\"], lookback_days=5)\n",
        "    features[\"decayed_sentiment\"] = float(decayed_sentiment)\n",
        "\n",
        "    return features\n",
        "\n",
        "# -------------------- Models & Features -------------------- #\n",
        "models = {\n",
        "    \"label_5d\": joblib.load(\"xgb_model_label_5d.pkl\"),\n",
        "    \"label_20d\": joblib.load(\"xgb_model_label_20d.pkl\"),\n",
        "    \"label_60d\": joblib.load(\"xgb_model_label_60d.pkl\"),\n",
        "}\n",
        "\n",
        "feature_cols = [\n",
        "    \"vol_5d\", \"vol_20d\", \"vol_60d\",\n",
        "    \"drawdown_60d\", \"de_ratio\",\n",
        "    \"prev_return_5d\", \"prev_return_20d\", \"prev_return_60d\",\n",
        "    \"decayed_sentiment\"\n",
        "]\n",
        "\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "# -------------------- Creditworthiness -------------------- #\n",
        "def creditworthiness_from_prob(prob: float) -> float:\n",
        "    prob = np.clip(prob, 1e-6, 1 - 1e-6)\n",
        "    score = 800 / (1 + np.exp(5 * (prob - 0.5)))\n",
        "    score = 300 + (score / 800) * 550\n",
        "    return round(score, 2)\n",
        "\n",
        "def calculate_creditworthiness_with_explain(features: dict, method: str = \"weighted\"):\n",
        "    df = pd.DataFrame([features])\n",
        "    X_scaled = scaler.transform(df[feature_cols])\n",
        "\n",
        "    probs, shap_metadata = {}, {}\n",
        "    for label, model in models.items():\n",
        "        prob = model.predict_proba(X_scaled)[:, 1][0]\n",
        "        probs[label] = float(prob)\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X_scaled)\n",
        "        shap_metadata[label] = {\n",
        "            \"base_value\": float(explainer.expected_value),\n",
        "            \"feature_values\": {f: float(v) for f, v in zip(feature_cols, X_scaled[0])},\n",
        "            \"shap_values\": {f: float(v) for f, v in zip(feature_cols, shap_values[0])}\n",
        "        }\n",
        "\n",
        "    if method == \"weighted\":\n",
        "        weights = {\"label_5d\": 0.3, \"label_20d\": 0.4, \"label_60d\": 0.3}\n",
        "        avg_prob = sum(probs[label] * weights[label] for label in probs)\n",
        "    else:\n",
        "        avg_prob = np.mean(list(probs.values()))\n",
        "\n",
        "    creditworthiness = creditworthiness_from_prob(avg_prob)\n",
        "    return creditworthiness, probs, shap_metadata\n",
        "\n",
        "def calculate_creditworthiness_with_explain_json(features: dict, method: str = \"weighted\"):\n",
        "    # Keep strings as-is, numeric as float\n",
        "    features = {k: float(v) if isinstance(v, (int, float, np.number)) else v for k, v in features.items()}\n",
        "    creditworthiness, probs, shap_metadata = calculate_creditworthiness_with_explain(features, method)\n",
        "    return json.dumps({\n",
        "        \"ticker\": features.get(\"ticker\", \"\"),\n",
        "        \"date\": features.get(\"date\", \"\"),\n",
        "        \"creditworthiness\": creditworthiness,\n",
        "        \"risk_probs\": probs,\n",
        "        \"shap_explanations\": shap_metadata\n",
        "    }, indent=2)\n",
        "\n",
        "# -------------------- Example -------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    ticker = \"AAPL\"\n",
        "    target_date = \"2025-03-20\"\n",
        "\n",
        "    print(f\"Fetching features for {ticker} on {target_date}...\")\n",
        "    features = get_ticker_features(ticker, target_date)\n",
        "    features[\"ticker\"] = ticker\n",
        "    print(\"Features fetched. Calculating creditworthiness...\")\n",
        "    result_json = calculate_creditworthiness_with_explain_json(features, method=\"weighted\")\n",
        "    print(result_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m99gZjFDIjj",
        "outputId": "26ec91ca-c6c7-4d09-b783-5e2cb1f6ee16"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching features for AAPL on 2025-03-20...\n",
            "Features fetched. Calculating creditworthiness...\n",
            "{\n",
            "  \"ticker\": \"AAPL\",\n",
            "  \"date\": \"2025-03-20\",\n",
            "  \"creditworthiness\": 788.62,\n",
            "  \"risk_probs\": {\n",
            "    \"label_5d\": 0.004004129208624363,\n",
            "    \"label_20d\": 0.17098811268806458,\n",
            "    \"label_60d\": 0.05164482071995735\n",
            "  },\n",
            "  \"shap_explanations\": {\n",
            "    \"label_5d\": {\n",
            "      \"base_value\": -0.02012277953326702,\n",
            "      \"feature_values\": {\n",
            "        \"vol_5d\": -0.3989725071591474,\n",
            "        \"vol_20d\": 0.08216323751156003,\n",
            "        \"vol_60d\": 0.048792744960614844,\n",
            "        \"drawdown_60d\": -1.3994656870166289,\n",
            "        \"de_ratio\": -0.5199108263249623,\n",
            "        \"prev_return_5d\": 0.37996425365089115,\n",
            "        \"prev_return_20d\": -1.6674786461599393,\n",
            "        \"prev_return_60d\": -1.2751907627448145,\n",
            "        \"decayed_sentiment\": -0.4519842699503451\n",
            "      },\n",
            "      \"shap_values\": {\n",
            "        \"vol_5d\": -0.5016213059425354,\n",
            "        \"vol_20d\": -0.5121458172798157,\n",
            "        \"vol_60d\": -0.882170557975769,\n",
            "        \"drawdown_60d\": -0.8724826574325562,\n",
            "        \"de_ratio\": -1.1570810079574585,\n",
            "        \"prev_return_5d\": -0.12888044118881226,\n",
            "        \"prev_return_20d\": -0.7043902277946472,\n",
            "        \"prev_return_60d\": -0.6251748204231262,\n",
            "        \"decayed_sentiment\": -0.11234646290540695\n",
            "      }\n",
            "    },\n",
            "    \"label_20d\": {\n",
            "      \"base_value\": -0.009759572334587574,\n",
            "      \"feature_values\": {\n",
            "        \"vol_5d\": -0.3989725071591474,\n",
            "        \"vol_20d\": 0.08216323751156003,\n",
            "        \"vol_60d\": 0.048792744960614844,\n",
            "        \"drawdown_60d\": -1.3994656870166289,\n",
            "        \"de_ratio\": -0.5199108263249623,\n",
            "        \"prev_return_5d\": 0.37996425365089115,\n",
            "        \"prev_return_20d\": -1.6674786461599393,\n",
            "        \"prev_return_60d\": -1.2751907627448145,\n",
            "        \"decayed_sentiment\": -0.4519842699503451\n",
            "      },\n",
            "      \"shap_values\": {\n",
            "        \"vol_5d\": -0.27064216136932373,\n",
            "        \"vol_20d\": -0.22276270389556885,\n",
            "        \"vol_60d\": 0.09491416811943054,\n",
            "        \"drawdown_60d\": 0.3834073543548584,\n",
            "        \"de_ratio\": -0.6192553043365479,\n",
            "        \"prev_return_5d\": -0.0652107298374176,\n",
            "        \"prev_return_20d\": -0.4246293902397156,\n",
            "        \"prev_return_60d\": -0.5709353089332581,\n",
            "        \"decayed_sentiment\": 0.12623260915279388\n",
            "      }\n",
            "    },\n",
            "    \"label_60d\": {\n",
            "      \"base_value\": -0.007735217455774546,\n",
            "      \"feature_values\": {\n",
            "        \"vol_5d\": -0.3989725071591474,\n",
            "        \"vol_20d\": 0.08216323751156003,\n",
            "        \"vol_60d\": 0.048792744960614844,\n",
            "        \"drawdown_60d\": -1.3994656870166289,\n",
            "        \"de_ratio\": -0.5199108263249623,\n",
            "        \"prev_return_5d\": 0.37996425365089115,\n",
            "        \"prev_return_20d\": -1.6674786461599393,\n",
            "        \"prev_return_60d\": -1.2751907627448145,\n",
            "        \"decayed_sentiment\": -0.4519842699503451\n",
            "      },\n",
            "      \"shap_values\": {\n",
            "        \"vol_5d\": 0.06601884216070175,\n",
            "        \"vol_20d\": -0.28297266364097595,\n",
            "        \"vol_60d\": -0.7169073820114136,\n",
            "        \"drawdown_60d\": -0.33457010984420776,\n",
            "        \"de_ratio\": -0.24928361177444458,\n",
            "        \"prev_return_5d\": -0.045231837779283524,\n",
            "        \"prev_return_20d\": -0.18586327135562897,\n",
            "        \"prev_return_60d\": -1.0100820064544678,\n",
            "        \"decayed_sentiment\": -0.1437123864889145\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KEmTMPFsU57d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsF_821nf_X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_inference.py\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import joblib\n",
        "import shap\n",
        "from gnews import GNews\n",
        "from transformers import pipeline\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "training_df = pd.read_csv(\"final training.csv\", parse_dates=[\"date\"])\n",
        "\n",
        "# -------------------- Fundamentals -------------------- #\n",
        "def fetch_fundamentals(ticker: str) -> dict:\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return {\n",
        "            \"de_ratio\": info.get(\"debtToEquity\", 0.0),\n",
        "            \"current_ratio\": info.get(\"currentRatio\", 0.0),\n",
        "            \"quick_ratio\": info.get(\"quickRatio\", 0.0),\n",
        "            \"roa\": info.get(\"returnOnAssets\", 0.0),\n",
        "            \"roe\": info.get(\"returnOnEquity\", 0.0),\n",
        "            \"profit_margin\": info.get(\"profitMargins\", 0.0),\n",
        "        }\n",
        "    except Exception:\n",
        "        return {\n",
        "            \"de_ratio\": 0.0, \"current_ratio\": 0.0, \"quick_ratio\": 0.0,\n",
        "            \"roa\": 0.0, \"roe\": 0.0, \"profit_margin\": 0.0\n",
        "        }\n",
        "\n",
        "# -------------------- Sentiment & News -------------------- #\n",
        "def get_company_name(stock_ticker: str) -> str:\n",
        "    try:\n",
        "        company = yf.Ticker(stock_ticker)\n",
        "        return company.info.get(\"longName\", \"\")\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def fetch_company_news(company_name: str, date: str, window: int = 3, max_articles: int = 5) -> list:\n",
        "    try:\n",
        "        google_news = GNews(language=\"en\", country=\"US\", max_results=max_articles)\n",
        "        target_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "        start_date = target_date - timedelta(days=window)\n",
        "        end_date = target_date + timedelta(days=window)\n",
        "\n",
        "        if start_date.date() == end_date.date():\n",
        "            end_date += timedelta(days=1)\n",
        "\n",
        "        google_news.start_date = (start_date.year, start_date.month, start_date.day)\n",
        "        google_news.end_date = (end_date.year, end_date.month, end_date.day)\n",
        "\n",
        "        news_results = google_news.get_news(company_name)\n",
        "        return news_results[:max_articles] if news_results else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def analyze_sentiment_with_hf(news_data: list) -> float:\n",
        "    if not news_data:\n",
        "        return 0.0\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
        "    )\n",
        "    total_score, count = 0, 0\n",
        "    for article in news_data:\n",
        "        try:\n",
        "            text_to_analyze = f\"{article.get('title', '')}. {article.get('description', '')}\"\n",
        "            sentiment = sentiment_pipeline(text_to_analyze)[0]\n",
        "            label, score = sentiment[\"label\"].upper(), sentiment[\"score\"]\n",
        "            mapped = score if label == \"POSITIVE\" else -score if label == \"NEGATIVE\" else 0.0\n",
        "            total_score += mapped\n",
        "            count += 1\n",
        "        except Exception:\n",
        "            continue\n",
        "    return round(total_score / count, 4) if count > 0 else 0.0\n",
        "\n",
        "# -------------------- Decayed Sentiment -------------------- #\n",
        "def sentiment_decay(dates, sentiments, decay=0.8, scale=1.0):\n",
        "    df = pd.DataFrame({\"date\": pd.to_datetime(dates), \"sentiment\": sentiments})\n",
        "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "    decayed_scores, prev_score = [], 0\n",
        "    for _, row in df.iterrows():\n",
        "        s = row[\"sentiment\"]\n",
        "        new_impact = s * scale\n",
        "        current_score = prev_score * decay + new_impact\n",
        "        decayed_scores.append(current_score)\n",
        "        prev_score = current_score\n",
        "    df[\"decayed_score\"] = decayed_scores\n",
        "    return df\n",
        "\n",
        "def compute_sentiment_features(ticker: str, target_date: str, lookback_days: int = 5) -> tuple[float, float]:\n",
        "    company_name = get_company_name(ticker)\n",
        "    if not company_name:\n",
        "        return 0.0, 0.0\n",
        "    dates, sentiments = [], []\n",
        "    for i in range(lookback_days, -1, -1):\n",
        "        day = (pd.to_datetime(target_date) - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
        "        news = fetch_company_news(company_name, day, window=0, max_articles=5)\n",
        "        score = analyze_sentiment_with_hf(news)\n",
        "        dates.append(day)\n",
        "        sentiments.append(score)\n",
        "\n",
        "    df = sentiment_decay(dates, sentiments, decay=0.8, scale=1.0)\n",
        "    decayed_score = float(df[\"decayed_score\"].iloc[-1])\n",
        "    latest_raw_score = float(df[\"sentiment\"].iloc[-1])\n",
        "    return decayed_score, latest_raw_score\n",
        "\n",
        "# -------------------- Technicals -------------------- #\n",
        "def safe_get(series, date):\n",
        "    val = series.loc[date]\n",
        "    if isinstance(val, pd.Series):\n",
        "        return float(val.iloc[-1] if not val.empty else 0.0)\n",
        "    return float(val or 0.0)\n",
        "\n",
        "def compute_features_for_inference(df: pd.DataFrame, fundamentals: dict, target_date: str) -> dict:\n",
        "    df = df.sort_index()\n",
        "    df = df[~df.index.duplicated(keep='last')]\n",
        "    close = df[\"Adj Close\"]\n",
        "    dt_target = pd.to_datetime(target_date)\n",
        "\n",
        "    df_filtered = df[df.index <= dt_target]\n",
        "    if df_filtered.empty:\n",
        "        raise ValueError(f\"No trading data available on or before {target_date}\")\n",
        "    actual_date = df_filtered.index.max()\n",
        "\n",
        "    vol_5d = close.pct_change().rolling(5).std()\n",
        "    vol_20d = close.pct_change().rolling(20).std()\n",
        "    vol_60d = close.pct_change().rolling(60).std()\n",
        "    rolling_max = close.rolling(60, min_periods=1).max()\n",
        "    drawdown_60d = close / rolling_max - 1.0\n",
        "    prev_return_5d = close.pct_change(5)\n",
        "    prev_return_20d = close.pct_change(20)\n",
        "    prev_return_60d = close.pct_change(60)\n",
        "\n",
        "    features = {\n",
        "        \"vol_5d\": safe_get(vol_5d, actual_date),\n",
        "        \"vol_20d\": safe_get(vol_20d, actual_date),\n",
        "        \"vol_60d\": safe_get(vol_60d, actual_date),\n",
        "        \"drawdown_60d\": safe_get(drawdown_60d, actual_date),\n",
        "        \"prev_return_5d\": safe_get(prev_return_5d, actual_date),\n",
        "        \"prev_return_20d\": safe_get(prev_return_20d, actual_date),\n",
        "        \"prev_return_60d\": safe_get(prev_return_60d, actual_date),\n",
        "    }\n",
        "\n",
        "    # Include fundamentals\n",
        "    features.update({k: float(v or 0.0) for k, v in fundamentals.items()})\n",
        "\n",
        "    features[\"date\"] = actual_date.strftime('%Y-%m-%d')\n",
        "    return features\n",
        "\n",
        "def get_ticker_features(ticker: str, target_date: str, lookback_years: int = 2) -> dict:\n",
        "    dt_target = pd.to_datetime(target_date)\n",
        "\n",
        "    # Check if ticker & date exist in training CSV\n",
        "    row = training_df[(training_df[\"ticker\"] == ticker) & (training_df[\"date\"] == dt_target)]\n",
        "    if not row.empty:\n",
        "        # Extract features directly from CSV\n",
        "        features = row.iloc[0].to_dict()\n",
        "        print(\"using csv\")\n",
        "        features['date'] = features['date'].strftime('%Y-%m-%d')\n",
        "        # Ensure numeric fields are floats\n",
        "        features = {k: float(v) if isinstance(v, (int, float, np.number)) else v for k, v in features.items()}\n",
        "        return features\n",
        "\n",
        "    # Otherwise, fetch from yfinance and compute\n",
        "    df = yf.download(ticker, period=f\"{lookback_years}y\", interval=\"1d\", auto_adjust=False, progress=False)\n",
        "    fundamentals = fetch_fundamentals(ticker)\n",
        "    features = compute_features_for_inference(df, fundamentals, target_date)\n",
        "\n",
        "    # Add sentiment\n",
        "    decayed_sentiment, _ = compute_sentiment_features(ticker, features[\"date\"], lookback_days=5)\n",
        "    features[\"decayed_sentiment\"] = float(decayed_sentiment)\n",
        "    return features\n",
        "\n",
        "# -------------------- Models & Features -------------------- #\n",
        "models = {\n",
        "    \"label_5d\": joblib.load(\"xgb_model_label_5d.pkl\"),\n",
        "    \"label_20d\": joblib.load(\"xgb_model_label_20d.pkl\"),\n",
        "    \"label_60d\": joblib.load(\"xgb_model_label_60d.pkl\"),\n",
        "}\n",
        "\n",
        "feature_cols = [\n",
        "    \"vol_5d\", \"vol_20d\", \"vol_60d\",\n",
        "    \"drawdown_60d\", \"de_ratio\",\n",
        "    \"prev_return_5d\", \"prev_return_20d\", \"prev_return_60d\",\n",
        "    \"decayed_sentiment\"\n",
        "]\n",
        "\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "# -------------------- Creditworthiness -------------------- #\n",
        "def creditworthiness_from_prob(prob: float) -> float:\n",
        "    prob = np.clip(prob, 1e-6, 1 - 1e-6)\n",
        "    score = 800 / (1 + np.exp(5 * (prob - 0.5)))\n",
        "    score = 300 + (score / 800) * 550\n",
        "    return round(score, 2)\n",
        "\n",
        "def calculate_creditworthiness_with_explain(features: dict, method: str = \"weighted\"):\n",
        "    df = pd.DataFrame([features])\n",
        "    X_scaled = scaler.transform(df[feature_cols])\n",
        "\n",
        "    probs, shap_metadata = {}, {}\n",
        "    for label, model in models.items():\n",
        "        prob = model.predict_proba(X_scaled)[:, 1][0]\n",
        "        probs[label] = float(prob)\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X_scaled)\n",
        "        shap_metadata[label] = {\n",
        "            \"base_value\": float(explainer.expected_value),\n",
        "            \"feature_values\": {f: float(v) for f, v in zip(feature_cols, X_scaled[0])},\n",
        "            \"shap_values\": {f: float(v) for f, v in zip(feature_cols, shap_values[0])}\n",
        "        }\n",
        "\n",
        "    if method == \"weighted\":\n",
        "        weights = {\"label_5d\": 0.3, \"label_20d\": 0.4, \"label_60d\": 0.3}\n",
        "        avg_prob = sum(probs[label] * weights[label] for label in probs)\n",
        "    else:\n",
        "        avg_prob = np.mean(list(probs.values()))\n",
        "\n",
        "    creditworthiness = creditworthiness_from_prob(avg_prob)\n",
        "    return creditworthiness, probs, shap_metadata\n",
        "\n",
        "def calculate_creditworthiness_with_explain_json(features: dict, method: str = \"weighted\"):\n",
        "    # Keep strings as-is, numeric as float\n",
        "    features = {k: float(v) if isinstance(v, (int, float, np.number)) else v for k, v in features.items()}\n",
        "    creditworthiness, probs, shap_metadata = calculate_creditworthiness_with_explain(features, method)\n",
        "    return json.dumps({\n",
        "        \"ticker\": features.get(\"ticker\", \"\"),\n",
        "        \"date\": features.get(\"date\", \"\"),\n",
        "        \"creditworthiness\": creditworthiness,\n",
        "        \"risk_probs\": probs,\n",
        "        \"shap_explanations\": shap_metadata\n",
        "    }, indent=2)\n",
        "\n",
        "# -------------------- Example -------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    ticker = \"AAPL\"\n",
        "    target_date = \"2025-08-22\"\n",
        "\n",
        "    print(f\"Fetching features for {ticker} on {target_date}...\")\n",
        "    features = get_ticker_features(ticker, target_date)\n",
        "    features[\"ticker\"] = ticker\n",
        "    print(\"Features fetched. Calculating creditworthiness...\")\n",
        "    result_json = calculate_creditworthiness_with_explain_json(features, method=\"weighted\")\n",
        "    print(result_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3YNVCP0f_oV",
        "outputId": "87a42744-9ef2-4d50-a828-868eea3cfcdd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching features for AAPL on 2025-08-22...\n",
            "Features fetched. Calculating creditworthiness...\n",
            "{\n",
            "  \"ticker\": \"AAPL\",\n",
            "  \"date\": \"2025-08-22\",\n",
            "  \"creditworthiness\": 795.67,\n",
            "  \"risk_probs\": {\n",
            "    \"label_5d\": 0.00023433212481904775,\n",
            "    \"label_20d\": 0.01304598804563284,\n",
            "    \"label_60d\": 0.17518840730190277\n",
            "  },\n",
            "  \"shap_explanations\": {\n",
            "    \"label_5d\": {\n",
            "      \"base_value\": -0.02012277953326702,\n",
            "      \"feature_values\": {\n",
            "        \"vol_5d\": -0.41091570826948537,\n",
            "        \"vol_20d\": 0.22536989182690628,\n",
            "        \"vol_60d\": -0.44502199806956755,\n",
            "        \"drawdown_60d\": 0.5694536865537014,\n",
            "        \"de_ratio\": -0.5199108263249623,\n",
            "        \"prev_return_5d\": -0.53698655137329,\n",
            "        \"prev_return_20d\": 0.5190814216414819,\n",
            "        \"prev_return_60d\": 0.6127477575883555,\n",
            "        \"decayed_sentiment\": -0.4519842699503451\n",
            "      },\n",
            "      \"shap_values\": {\n",
            "        \"vol_5d\": -1.6020745038986206,\n",
            "        \"vol_20d\": -0.9940552115440369,\n",
            "        \"vol_60d\": -2.145747423171997,\n",
            "        \"drawdown_60d\": -1.1226099729537964,\n",
            "        \"de_ratio\": -1.2966893911361694,\n",
            "        \"prev_return_5d\": -0.10850410908460617,\n",
            "        \"prev_return_20d\": -0.5290840268135071,\n",
            "        \"prev_return_60d\": -0.24684257805347443,\n",
            "        \"decayed_sentiment\": -0.2928067743778229\n",
            "      }\n",
            "    },\n",
            "    \"label_20d\": {\n",
            "      \"base_value\": -0.009759572334587574,\n",
            "      \"feature_values\": {\n",
            "        \"vol_5d\": -0.41091570826948537,\n",
            "        \"vol_20d\": 0.22536989182690628,\n",
            "        \"vol_60d\": -0.44502199806956755,\n",
            "        \"drawdown_60d\": 0.5694536865537014,\n",
            "        \"de_ratio\": -0.5199108263249623,\n",
            "        \"prev_return_5d\": -0.53698655137329,\n",
            "        \"prev_return_20d\": 0.5190814216414819,\n",
            "        \"prev_return_60d\": 0.6127477575883555,\n",
            "        \"decayed_sentiment\": -0.4519842699503451\n",
            "      },\n",
            "      \"shap_values\": {\n",
            "        \"vol_5d\": 0.008301381021738052,\n",
            "        \"vol_20d\": -0.46461838483810425,\n",
            "        \"vol_60d\": -1.1027886867523193,\n",
            "        \"drawdown_60d\": 0.11095333099365234,\n",
            "        \"de_ratio\": -0.987230122089386,\n",
            "        \"prev_return_5d\": -0.4513901472091675,\n",
            "        \"prev_return_20d\": -0.02174776792526245,\n",
            "        \"prev_return_60d\": -0.880478024482727,\n",
            "        \"decayed_sentiment\": -0.5273848176002502\n",
            "      }\n",
            "    },\n",
            "    \"label_60d\": {\n",
            "      \"base_value\": -0.007735217455774546,\n",
            "      \"feature_values\": {\n",
            "        \"vol_5d\": -0.41091570826948537,\n",
            "        \"vol_20d\": 0.22536989182690628,\n",
            "        \"vol_60d\": -0.44502199806956755,\n",
            "        \"drawdown_60d\": 0.5694536865537014,\n",
            "        \"de_ratio\": -0.5199108263249623,\n",
            "        \"prev_return_5d\": -0.53698655137329,\n",
            "        \"prev_return_20d\": 0.5190814216414819,\n",
            "        \"prev_return_60d\": 0.6127477575883555,\n",
            "        \"decayed_sentiment\": -0.4519842699503451\n",
            "      },\n",
            "      \"shap_values\": {\n",
            "        \"vol_5d\": -0.04078497365117073,\n",
            "        \"vol_20d\": 0.012566184625029564,\n",
            "        \"vol_60d\": -0.9616991281509399,\n",
            "        \"drawdown_60d\": -0.07272464036941528,\n",
            "        \"de_ratio\": -0.3182256817817688,\n",
            "        \"prev_return_5d\": -0.09065511077642441,\n",
            "        \"prev_return_20d\": -0.016551747918128967,\n",
            "        \"prev_return_60d\": -0.0059157563373446465,\n",
            "        \"decayed_sentiment\": -0.047566529363393784\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A0ztEyyMkCLy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}